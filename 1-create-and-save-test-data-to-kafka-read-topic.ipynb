{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e341e0b5-18ce-45c0-8540-f747cb4b4d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook will create a simulated data and push to Kafka queue named \"rtm_read_demo\" in realtime mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b40181-0ac7-44b6-a5c2-095198a4d209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kafka-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eafc792-442e-4270-80c7-deaf7233f0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kafka_user = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka_user\")\n",
    "kafka_password = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka_password\")\n",
    "kafka_topic=dbutils.secrets.get(scope=\"rtm_demo\", key=\"topic\")\n",
    "kafka_bootstrap_servers=dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka.bootstrap.servers\")\n",
    "\n",
    "jaas_config = (\n",
    "    'kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule required '\n",
    "    f'username=\"{kafka_user}\" '\n",
    "    f'password=\"{kafka_password}\";'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7380fbb-251a-4381-a07d-e901a15afb5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0. IMPORTS\n",
    "# ============================================================\n",
    "from pyspark.sql.functions import col, rand, round as spark_round, expr, current_timestamp, to_json, struct\n",
    "import uuid as python_uuid\n",
    "import random\n",
    "\n",
    "\n",
    "# Make sure kafka-python is installed on the cluster. [web:26][web:18]\n",
    "\n",
    "# ============================================================\n",
    "# 1. NOTEBOOK PARAMETERS (DATABRICKS WIDGETS)\n",
    "# ============================================================\n",
    "# Get Kafka connection details from secrets (defined in Cell 3)\n",
    "kafka_bootstrap_servers = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka.bootstrap.servers\")\n",
    "kafka_topic = dbutils.secrets.get(scope=\"rtm_demo\", key=\"topic\")\n",
    "\n",
    "# Create these once in a separate cell, then re-use:\n",
    "# dbutils.widgets.text(\"kafka_bootstrap\", \"broker1:9092\", \"Kafka bootstrap servers\")\n",
    "# dbutils.widgets.text(\"kafka_topic\", \"customer_txn_topic\", \"Kafka topic\")\n",
    "dbutils.widgets.text(\"kafka_partitions\", \"1\", \"Kafka partitions\")\n",
    "dbutils.widgets.text(\"rows_per_second\", \"10\", \"Rows per second\")\n",
    "dbutils.widgets.dropdown(\"duplication_mode\", \"none\", [\"none\", \"percentage\"], \"Duplication mode\")\n",
    "dbutils.widgets.text(\"duplication_value\", \"0.0\", \"Duplication value\")\n",
    "\n",
    "#kafka_bootstrap   = dbutils.widgets.get(\"kafka_bootstrap\")\n",
    "#kafka_topic       = dbutils.widgets.get(\"kafka_topic\")\n",
    "kafka_partitions  = int(dbutils.widgets.get(\"kafka_partitions\"))\n",
    "rows_per_second   = int(dbutils.widgets.get(\"rows_per_second\"))\n",
    "duplication_mode  = dbutils.widgets.get(\"duplication_mode\")      # \"none\" | \"percentage\" | \"multiplier\"\n",
    "duplication_value = dbutils.widgets.get(\"duplication_value\")\n",
    "\n",
    "if duplication_mode == \"percentage\":\n",
    "    duplication_percentage = float(duplication_value)   # e.g. 0.30\n",
    "else:\n",
    "    duplication_percentage = 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6766363f-9e8f-446f-ad04-fd2c0764407c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import UnknownTopicOrPartitionError, NoBrokersAvailable\n",
    "try:\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers= kafka_bootstrap_servers,  \n",
    "        client_id=\"databricks-admin-client\",\n",
    "        security_protocol=\"SASL_SSL\",\n",
    "        sasl_mechanism=\"SCRAM-SHA-256\",\n",
    "        sasl_plain_username=kafka_user,\n",
    "        sasl_plain_password=kafka_password\n",
    "    )\n",
    "except NoBrokersAvailable as e:\n",
    "    raise RuntimeError(f\"Cannot connect to Redpanda at {kafka_bootstrap_servers}: {e}\")\n",
    "\n",
    "# Delete topic if it exists\n",
    "try:\n",
    "    admin_client.delete_topics(\n",
    "        topics=[kafka_topic],\n",
    "        timeout_ms=30000\n",
    "    )\n",
    "    print(f\"Requested deletion of topic: {kafka_topic}\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(f\"Topic {kafka_topic} did not exist, skipping delete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Delete topics error for {kafka_topic}: {e}\")\n",
    "\n",
    "# Create topic\n",
    "new_topic = NewTopic(\n",
    "    name=kafka_topic,\n",
    "    num_partitions=kafka_partitions,\n",
    "    replication_factor=3\n",
    ")\n",
    "\n",
    "try:\n",
    "    admin_client.create_topics(\n",
    "        new_topics=[new_topic],\n",
    "        validate_only=False\n",
    "    )\n",
    "    print(f\"Topic {kafka_topic} created with {kafka_partitions} partitions.\")\n",
    "except Exception as e:\n",
    "    print(f\"Create topic error for {kafka_topic}: {e}\")\n",
    "finally:\n",
    "    admin_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762e0220-7a20-40ce-b77f-54866ab136a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, expr, current_timestamp, to_json, struct, lit\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import uuid as python_uuid\n",
    "import random\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "print(\"Config values:\")\n",
    "print(f\"  duplication_mode: {duplication_mode}\")           # \"none\" or \"duplicate\"\n",
    "print(f\"  duplication_value: {duplication_value}\")\n",
    "print(f\"  rows_per_second: {rows_per_second}\")\n",
    "print(f\"  kafka_partitions: {kafka_partitions}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 1. PRECOMPUTE STATIC EVENT POOL (BATCH)\n",
    "# ============================================================\n",
    "\n",
    "num_events = 1000\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "events = []\n",
    "for i in range(num_events):\n",
    "    cid = str(python_uuid.uuid4())\n",
    "    amount = round(random.random() * 180 + 20, 2)  # 20–200\n",
    "    base_std = random.random() * 25 + 5\n",
    "    daily_avg = round(amount + (base_std if random.random() > 0.5 else -base_std), 2)\n",
    "    events.append((i, cid, amount, daily_avg))\n",
    "\n",
    "events_schema = StructType([\n",
    "    StructField(\"event_index\",   IntegerType(), False),\n",
    "    StructField(\"customerID\",    StringType(), False),\n",
    "    StructField(\"amount\",        DoubleType(), False),\n",
    "    StructField(\"daily_average\", DoubleType(), False),\n",
    "])\n",
    "\n",
    "events_dim = spark.createDataFrame(events, schema=events_schema).cache()\n",
    "\n",
    "print(f\"Precomputed event pool size: {events_dim.count()}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. RATE STREAM\n",
    "# ============================================================\n",
    "\n",
    "rate_df = (\n",
    "    spark.readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"numPartitions\", kafka_partitions)\n",
    "    .option(\"rowsPerSecond\", rows_per_second)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3. MAP RATE VALUES TO EVENT INDICES\n",
    "# ============================================================\n",
    "\n",
    "base_idx_df = (\n",
    "    rate_df\n",
    "    .withColumn(\"event_index\", (col(\"value\") % num_events).cast(\"int\"))\n",
    ")\n",
    "\n",
    "if duplication_mode == \"duplicate\":\n",
    "    dup_idx_df = (\n",
    "        base_idx_df\n",
    "        .select(\n",
    "            col(\"timestamp\").alias(\"timestamp_dup\"),\n",
    "            col(\"event_index\")\n",
    "        )\n",
    "        .withColumn(\"dup_flag\", lit(True))\n",
    "    )\n",
    "\n",
    "    base_idx_df = base_idx_df.withColumn(\"dup_flag\", lit(False))\n",
    "\n",
    "    idx_union_df = (\n",
    "        base_idx_df\n",
    "        .select(col(\"timestamp\"), col(\"event_index\"), col(\"dup_flag\"))\n",
    "        .unionByName(\n",
    "            dup_idx_df.select(\n",
    "                col(\"timestamp_dup\").alias(\"timestamp\"),\n",
    "                col(\"event_index\"),\n",
    "                col(\"dup_flag\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    idx_union_df = base_idx_df.withColumn(\"dup_flag\", lit(False))\n",
    "\n",
    "# ============================================================\n",
    "# 4. JOIN WITH STATIC EVENTS TO GET FIELDS\n",
    "# ============================================================\n",
    "\n",
    "txn_df = (\n",
    "    idx_union_df\n",
    "    .join(events_dim, on=\"event_index\", how=\"inner\")\n",
    "    .withColumn(\"timestamp\", current_timestamp())\n",
    "    .select(\"customerID\", \"timestamp\", \"amount\", \"daily_average\", \"dup_flag\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5. WRITE TO KAFKA\n",
    "# ============================================================\n",
    "\n",
    "kafka_df = (\n",
    "    txn_df\n",
    "    .select(\n",
    "        col(\"customerID\").cast(\"string\").alias(\"key\"),\n",
    "        to_json(\n",
    "            struct(\n",
    "                col(\"customerID\"),\n",
    "                col(\"timestamp\"),\n",
    "                col(\"amount\"),\n",
    "                col(\"daily_average\"),\n",
    "                col(\"dup_flag\")\n",
    "            )\n",
    "        ).alias(\"value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "query = (\n",
    "    kafka_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .queryName(\"customer_txn_kafka_dim_events\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "    .option(\"topic\", kafka_topic)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\n",
    "    .option(\"kafka.sasl.jaas.config\", jaas_config)\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/customer_txn_kafka_dim_events_a\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(f\"Streaming query started to topic: {kafka_topic}\")\n",
    "print(f\"Query ID: {query.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ad4cde-2088-4296-aa03-93949fe5ab3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(duplicates_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7311391295448196,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1-create-and-save-test-data-to-kafka-read-topic",
   "widgets": {
    "duplication_mode": {
     "currentValue": "percentage",
     "nuid": "8da7c986-9077-49df-83cf-b4c2bc2ddccb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "none",
      "label": "Duplication mode",
      "name": "duplication_mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "none",
        "percentage"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "none",
      "label": "Duplication mode",
      "name": "duplication_mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "none",
        "percentage"
       ]
      }
     }
    },
    "duplication_value": {
     "currentValue": "10",
     "nuid": "8db73c25-4acc-4896-9e9e-e6ff33bddbf9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.0",
      "label": "Duplication value",
      "name": "duplication_value",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.0",
      "label": "Duplication value",
      "name": "duplication_value",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "kafka_partitions": {
     "currentValue": "4",
     "nuid": "d05190db-0aac-451c-a35f-6629c7f46c07",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": "Kafka partitions",
      "name": "kafka_partitions",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": "Kafka partitions",
      "name": "kafka_partitions",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rows_per_second": {
     "currentValue": "10000",
     "nuid": "877346f0-d6d1-44e0-9a03-e54bf9e7f574",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": "Rows per second",
      "name": "rows_per_second",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": "Rows per second",
      "name": "rows_per_second",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
