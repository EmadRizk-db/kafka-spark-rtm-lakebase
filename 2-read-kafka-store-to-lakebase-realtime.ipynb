{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e66d4c-682a-4a02-a1d5-38651437d4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install jaydebeapi\n",
    "%pip install SQLAlchemy\n",
    "dbutils.library.restartPython()\n",
    "#You will also need to install postgres jdbc jar and make it available in the classpath\n",
    "#The version used in this notebook is postgresql-42.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31884290-92c5-4f9a-9595-f384c787ff41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - set these values directly\n",
    "\n",
    "\n",
    "JDBC_URL = dbutils.secrets.get(scope=\"rtm_demo\", key=\"db_url\")\n",
    "JDBC_USER = dbutils.secrets.get(scope=\"rtm_demo\", key=\"db_user\")\n",
    "JDBC_PASSWORD = dbutils.secrets.get(scope=\"rtm_demo\", key=\"db_password\")\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "JDBC_JAR_PATH = dbutils.secrets.get(scope=\"rtm_demo\", key=\"jdbc_driver_jar_path\")\n",
    "TABLE_NAME = \"transactions_stream\"\n",
    "kafka_user = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka_user\")\n",
    "kafka_password = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka_password\")\n",
    "kafka_topic = dbutils.secrets.get(scope=\"rtm_demo\", key=\"topic\")\n",
    "jaas_config = (\n",
    "    'kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule required '\n",
    "    f'username=\"{kafka_user}\" '\n",
    "    f'password=\"{kafka_password}\";'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2106f43-37ae-49ea-9d89-86ebb6e43bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka.bootstrap.servers\"))\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"latest\")          # or \"earliest\"\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\n",
    "    .option(\"kafka.sasl.jaas.config\", jaas_config)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Decode key/value to strings\n",
    "events_df = (\n",
    "    raw_kafka_df\n",
    "    .selectExpr(\n",
    "        \"CAST(key AS STRING) AS customerID\",\n",
    "        \"CAST(value AS STRING) AS value_json\",\n",
    "        \"timestamp\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f8a9c8-2d41-498d-8456-75254da60312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffffd3f8-7e41-4d8b-8506-571e63f1930a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool as pg_pool\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def parse_pg_url(jdbc_url, jdbc_user, jdbc_password):\n",
    "    \"\"\"\n",
    "    Parse jdbc_url into connection kwargs for psycopg2.\n",
    "    Supports jdbc:postgresql://host:port/db or postgres://host:port/db.\n",
    "    \"\"\"\n",
    "    url = jdbc_url.replace(\"jdbc:\", \"\")  # strip jdbc: if present\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    return {\n",
    "        \"host\": parsed.hostname,\n",
    "        \"port\": parsed.port or 5432,\n",
    "        \"dbname\": parsed.path.lstrip(\"/\"),\n",
    "        \"user\": jdbc_user,\n",
    "        \"password\": jdbc_password,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_pg_writer(jdbc_url, jdbc_user, jdbc_password, jdbc_driver,\n",
    "                   jdbc_jar_path, table_name, pool_size):\n",
    "    conn_kwargs = parse_pg_url(jdbc_url, jdbc_user, jdbc_password)\n",
    "\n",
    "    # ========= 1) SIMPLE WRITER =========\n",
    "    class SimplePgForeachWriter:\n",
    "        def open(self, partition_id, epoch_id):\n",
    "            try:\n",
    "                self.conn = psycopg2.connect(**conn_kwargs)\n",
    "                self.conn.autocommit = True  # match previous autocommit behavior\n",
    "                self.cursor = self.conn.cursor()\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening psycopg2 connection: {e}\")\n",
    "                self.conn = None\n",
    "                self.cursor = None\n",
    "                return False\n",
    "\n",
    "        def process(self, row):\n",
    "            sql = f\"\"\"\n",
    "                INSERT INTO {table_name} (customer_id, value_json, event_ts)\n",
    "                VALUES (%s, %s::jsonb, %s::timestamp)\n",
    "            \"\"\"\n",
    "\n",
    "            customer_id = row[\"customerID\"]\n",
    "            value_json = row[\"value_json\"]\n",
    "            ts = row[\"timestamp\"]\n",
    "\n",
    "            if isinstance(ts, datetime):\n",
    "                ts_str = ts.isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "            else:\n",
    "                ts_str = str(ts)\n",
    "\n",
    "            self.cursor.execute(\n",
    "                sql,\n",
    "                (\n",
    "                    customer_id,\n",
    "                    value_json,\n",
    "                    ts_str,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        def close(self, error):\n",
    "            try:\n",
    "                if getattr(self, \"cursor\", None):\n",
    "                    self.cursor.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if getattr(self, \"conn\", None):\n",
    "                    self.conn.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return SimplePgForeachWriter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197d1a3f-aff2-4d19-ade2-cdf6bcbf7d0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "\n",
    "def parse_pg_url(jdbc_url, jdbc_user, jdbc_password):\n",
    "    url = jdbc_url.replace(\"jdbc:\", \"\")\n",
    "    parsed = urlparse(url)\n",
    "    return {\n",
    "        \"host\": parsed.hostname,\n",
    "        \"port\": parsed.port or 5432,\n",
    "        \"dbname\": parsed.path.lstrip(\"/\"),\n",
    "        \"user\": jdbc_user,\n",
    "        \"password\": jdbc_password,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_pg_buffered_writer(\n",
    "    jdbc_url,\n",
    "    jdbc_user,\n",
    "    jdbc_password,\n",
    "    jdbc_driver,\n",
    "    jdbc_jar_path,\n",
    "    table_name,\n",
    "    pool_size,\n",
    "    max_batch_size=100,   # flush when >= this many rows\n",
    "    flush_secs=2.0,       # flush if last flush older than this\n",
    "):\n",
    "    conn_kwargs = parse_pg_url(jdbc_url, jdbc_user, jdbc_password)\n",
    "\n",
    "    class BufferedPgForeachWriter:\n",
    "        def open(self, partition_id, epoch_id):\n",
    "            try:\n",
    "                print(f\"[JDBC] open partition={partition_id}, epoch={epoch_id}\")\n",
    "                self.conn = psycopg2.connect(**conn_kwargs)\n",
    "                self.conn.autocommit = False\n",
    "                self.cursor = self.conn.cursor()\n",
    "                self.buffer = []\n",
    "                self.last_flush_ts = time.time()\n",
    "                self.sql = (\n",
    "                    f\"INSERT INTO {table_name} (customer_id, value_json, event_ts) \"\n",
    "                    f\"VALUES (%s, %s::jsonb, %s::timestamp)\"\n",
    "                )\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error opening connection: {e}\")\n",
    "                self.conn = None\n",
    "                self.cursor = None\n",
    "                return False\n",
    "\n",
    "        def _flush_if_needed(self, force=False):\n",
    "            now = time.time()\n",
    "            need_by_size = len(self.buffer) >= max_batch_size\n",
    "            need_by_time = (now - self.last_flush_ts) >= flush_secs\n",
    "\n",
    "            if not (force or need_by_size or need_by_time):\n",
    "                return\n",
    "\n",
    "            if not self.buffer:\n",
    "                self.last_flush_ts = now\n",
    "                return\n",
    "\n",
    "            batch = list(self.buffer)\n",
    "            self.buffer.clear()\n",
    "            self.last_flush_ts = now\n",
    "\n",
    "            try:\n",
    "                print(f\"[JDBC] flushing {len(batch)} rows\")\n",
    "                execute_batch(self.cursor, self.sql, batch)\n",
    "                self.conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error flushing batch: {e}\")\n",
    "                # Put rows back so theyâ€™re not lost\n",
    "                self.buffer[:0] = batch\n",
    "                self.conn.rollback()\n",
    "                raise\n",
    "\n",
    "        def process(self, row):\n",
    "            print(f\"[JDBC] process row: {row}\")\n",
    "\n",
    "            customer_id = row[\"customerID\"]\n",
    "            value_json = row[\"value_json\"]\n",
    "            ts = row[\"timestamp\"]\n",
    "\n",
    "            if isinstance(ts, datetime):\n",
    "                ts_str = ts.isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "            else:\n",
    "                ts_str = str(ts)\n",
    "\n",
    "            self.buffer.append((customer_id, value_json, ts_str))\n",
    "            self._flush_if_needed(force=False)\n",
    "\n",
    "        def close(self, error):\n",
    "            print(f\"[JDBC] close, error={error}\")\n",
    "            try:\n",
    "                if error is None:\n",
    "                    self._flush_if_needed(force=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error flushing on close: {e}\")\n",
    "            try:\n",
    "                if getattr(self, \"cursor\", None):\n",
    "                    self.cursor.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if getattr(self, \"conn\", None):\n",
    "                    self.conn.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return BufferedPgForeachWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14e89ee-09e1-4ae8-824f-d8d6de6aa7b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "# Get widget parameters\n",
    "max_batch_size_param = int(dbutils.widgets.get(\"max_batch_size\"))\n",
    "flush_secs_param = float(dbutils.widgets.get(\"flush_secs\"))\n",
    "\n",
    "print(f\"Buffered writer configuration:\")\n",
    "print(f\"  max_batch_size: {max_batch_size_param}\")\n",
    "print(f\"  flush_secs: {flush_secs_param}\")\n",
    "print()\n",
    "\n",
    "jdbc_writer = make_pg_buffered_writer(\n",
    "    jdbc_url=JDBC_URL,\n",
    "    jdbc_user=JDBC_USER,\n",
    "    jdbc_password=JDBC_PASSWORD,\n",
    "    jdbc_driver=JDBC_DRIVER,\n",
    "    jdbc_jar_path=JDBC_JAR_PATH,\n",
    "    table_name=TABLE_NAME,\n",
    "    pool_size=None,\n",
    "    max_batch_size=max_batch_size_param,\n",
    "    flush_secs=flush_secs_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca9b020-7a04-41b7-a510-a47516f2f32f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "outputs": [],
   "source": [
    "jdbc_writer = make_pg_writer(\n",
    "    jdbc_url=JDBC_URL,\n",
    "    jdbc_user=JDBC_USER,\n",
    "    jdbc_password=JDBC_PASSWORD,\n",
    "    jdbc_driver=JDBC_DRIVER,\n",
    "    jdbc_jar_path=JDBC_JAR_PATH,\n",
    "    table_name=TABLE_NAME,\n",
    "    pool_size=POOL_SIZE if use_sqlalchemy_pool == \"yes\" else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1587fa55-b6b1-463e-99cf-07eaf5892a18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming query with optional deduplication"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Get the deduplication widget value\n",
    "deduplicate = dbutils.widgets.get(\"Deduplicate\")\n",
    "\n",
    "print(f\"Deduplication setting: {deduplicate}\")\n",
    "print()\n",
    "\n",
    "df_for_write = events_df  # Start with the base streaming DataFrame\n",
    "\n",
    "if deduplicate == \"Yes\":\n",
    "    print(\"Executing streaming query WITH deduplication...\")\n",
    "    print(\"  - Parsing JSON to extract amount and daily_average\")\n",
    "    print(\"  - Adding 10-minute watermark\")\n",
    "    print(\"  - Dropping duplicates based on customerID, amount, and daily_average\")\n",
    "    print()\n",
    "    \n",
    "    query = (\n",
    "        df_for_write    \n",
    "        .withColumn(\"parsed_json\", F.from_json(\"value_json\", \"amount STRING, daily_average STRING\"))\n",
    "        .withColumn(\"amount\", F.col(\"parsed_json.amount\"))\n",
    "        .withColumn(\"daily_average\", F.col(\"parsed_json.daily_average\"))\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "        .dropDuplicates([\"customerID\", \"amount\", \"daily_average\"])\n",
    "        .select(\"customerID\", \"value_json\", \"timestamp\")\n",
    "        .writeStream\n",
    "        .foreach(jdbc_writer)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(\"jdbc_sink_writer_deduplicated\")\n",
    "        .trigger(realTime=\"15 seconds\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/foreach_jdbc_checkpoint_deduplicated_8\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    print(f\"Deduplicated streaming query started: {query.id}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Executing streaming query WITHOUT deduplication...\")\n",
    "    print()\n",
    "    \n",
    "    query = (\n",
    "        df_for_write.writeStream\n",
    "        .foreach(jdbc_writer)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(\"jdbc_sink_writer\")\n",
    "        .trigger(realTime=\"15 seconds\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/foreach_jdbc_checkpoint_standard_8\")\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    print(f\"Standard streaming query started: {query.id}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2-read-kafka-store-to-lakebase-realtime",
   "widgets": {
    "Deduplicate": {
     "currentValue": "No",
     "nuid": "531ebbba-9f6c-4817-92a1-c17f61a7e360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "No",
      "label": null,
      "name": "Deduplicate",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "No",
        "Yes"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "No",
      "label": null,
      "name": "Deduplicate",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "No",
        "Yes"
       ]
      }
     }
    },
    "flush_secs": {
     "currentValue": "0.5",
     "nuid": "faf16f4a-9ec6-46f5-8ba0-1516118f835d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5.0",
      "label": "Flush Seconds",
      "name": "flush_secs",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5.0",
      "label": "Flush Seconds",
      "name": "flush_secs",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "max_batch_size": {
     "currentValue": "1000",
     "nuid": "0b44022a-6dbb-4d7b-a058-364fdbbe1ce0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "500",
      "label": "Max Batch Size",
      "name": "max_batch_size",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "500",
      "label": "Max Batch Size",
      "name": "max_batch_size",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
