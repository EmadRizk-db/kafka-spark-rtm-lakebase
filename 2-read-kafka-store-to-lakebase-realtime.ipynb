{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e66d4c-682a-4a02-a1d5-38651437d4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary\n",
    "%pip install kafka-python\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31884290-92c5-4f9a-9595-f384c787ff41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - set these values directly\n",
    "\n",
    "\n",
    "JDBC_URL = dbutils.secrets.get(scope=\"rtm_demo\", key=\"db_url\")\n",
    "JDBC_USER = dbutils.secrets.get(scope=\"rtm_demo\", key=\"db_user\")\n",
    "JDBC_PASSWORD = dbutils.secrets.get(scope=\"rtm_demo\", key=\"db_password\")\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "TABLE_NAME = \"transactions_stream\"\n",
    "kafka_user = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka_user\")\n",
    "kafka_password = dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka_password\")\n",
    "kafka_topic = dbutils.secrets.get(scope=\"rtm_demo\", key=\"topic\")\n",
    "jaas_config = (\n",
    "    'kafkashaded.org.apache.kafka.common.security.scram.ScramLoginModule required '\n",
    "    f'username=\"{kafka_user}\" '\n",
    "    f'password=\"{kafka_password}\";'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2106f43-37ae-49ea-9d89-86ebb6e43bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", dbutils.secrets.get(scope=\"rtm_demo\", key=\"kafka.bootstrap.servers\"))\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"latest\")          # or \"earliest\"\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\n",
    "    .option(\"kafka.sasl.jaas.config\", jaas_config)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Decode key/value to strings\n",
    "events_df = (\n",
    "    raw_kafka_df\n",
    "    .selectExpr(\n",
    "        \"CAST(key AS STRING) AS customerID\",\n",
    "        \"CAST(value AS STRING) AS value_json\",\n",
    "        \"timestamp\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f8a9c8-2d41-498d-8456-75254da60312",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Deduplicate widget\n",
    "deduplicate = dbutils.widgets.get(\"Deduplicate\")\n",
    "\n",
    "# Set shuffle partitions to match Kafka topic partition count\n",
    "# NOTE: This should match the kafka_partitions value used in the producer notebook\n",
    "# The producer creates the topic with 8 partitions\n",
    "num_partitions = 8\n",
    "\n",
    "print(f\"Setting shuffle partitions to {num_partitions} (matching Kafka topic partition count)\")\n",
    "\n",
    "if deduplicate == \"Yes\":\n",
    "    print(f\"Deduplication enabled: Using {num_partitions} shuffle partitions\")\n",
    "else:\n",
    "    print(f\"Deduplication disabled: Using {num_partitions} shuffle partitions\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", num_partitions)\n",
    "\n",
    "print(f\"\\nCurrent shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197d1a3f-aff2-4d19-ade2-cdf6bcbf7d0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "\n",
    "def parse_pg_url(jdbc_url, jdbc_user, jdbc_password):\n",
    "    url = jdbc_url.replace(\"jdbc:\", \"\")\n",
    "    parsed = urlparse(url)\n",
    "    return {\n",
    "        \"host\": parsed.hostname,\n",
    "        \"port\": parsed.port or 5432,\n",
    "        \"dbname\": parsed.path.lstrip(\"/\"),\n",
    "        \"user\": jdbc_user,\n",
    "        \"password\": jdbc_password,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_pg_buffered_writer(\n",
    "    jdbc_url,\n",
    "    jdbc_user,\n",
    "    jdbc_password,\n",
    "    jdbc_driver,\n",
    "    table_name,\n",
    "    pool_size,\n",
    "    max_batch_size=100,   # flush when >= this many rows (0 = simple mode)\n",
    "    flush_secs=2.0,       # flush if last flush older than this (0 = simple mode)\n",
    "):\n",
    "    conn_kwargs = parse_pg_url(jdbc_url, jdbc_user, jdbc_password)\n",
    "    \n",
    "    # Determine mode based on parameters\n",
    "    use_buffered_mode = max_batch_size > 0 and flush_secs > 0\n",
    "    \n",
    "    if use_buffered_mode:\n",
    "        print(f\"[JDBC] Using BUFFERED mode (max_batch_size={max_batch_size}, flush_secs={flush_secs})\")\n",
    "    else:\n",
    "        print(f\"[JDBC] Using SIMPLE mode (immediate writes)\")\n",
    "\n",
    "    class PgForeachWriter:\n",
    "        def open(self, partition_id, epoch_id):\n",
    "            try:\n",
    "                print(f\"[JDBC] open partition={partition_id}, epoch={epoch_id}\")\n",
    "                self.conn = psycopg2.connect(**conn_kwargs)\n",
    "                self.conn.autocommit = False\n",
    "                self.cursor = self.conn.cursor()\n",
    "                \n",
    "                # Schema will be set on first process() call\n",
    "                self.column_names = None\n",
    "                self.sql = None\n",
    "                \n",
    "                # Buffering state (only used in buffered mode)\n",
    "                if use_buffered_mode:\n",
    "                    self.buffer = []\n",
    "                    self.last_flush_ts = time.time()\n",
    "                \n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error opening connection: {e}\")\n",
    "                self.conn = None\n",
    "                self.cursor = None\n",
    "                return False\n",
    "\n",
    "        def _initialize_schema(self, row):\n",
    "            \"\"\"Initialize SQL statement based on row schema (called once per partition)\"\"\"\n",
    "            if self.column_names is not None:\n",
    "                return  # Already initialized\n",
    "            \n",
    "            # Get column names from the row\n",
    "            self.column_names = row.asDict().keys()\n",
    "            \n",
    "            # Build dynamic INSERT statement\n",
    "            columns_str = \", \".join(self.column_names)\n",
    "            placeholders = \", \".join([\"%s\" for _ in self.column_names])\n",
    "            \n",
    "            self.sql = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})\"\n",
    "            \n",
    "            print(f\"[JDBC] Initialized schema with columns: {list(self.column_names)}\")\n",
    "            print(f\"[JDBC] SQL: {self.sql}\")\n",
    "\n",
    "        def _flush_if_needed(self, force=False):\n",
    "            \"\"\"Flush buffer if needed (buffered mode only)\"\"\"\n",
    "            if not use_buffered_mode:\n",
    "                return  # Not applicable in simple mode\n",
    "            \n",
    "            now = time.time()\n",
    "            need_by_size = len(self.buffer) >= max_batch_size\n",
    "            need_by_time = (now - self.last_flush_ts) >= flush_secs\n",
    "\n",
    "            if not (force or need_by_size or need_by_time):\n",
    "                return\n",
    "\n",
    "            if not self.buffer:\n",
    "                self.last_flush_ts = now\n",
    "                return\n",
    "\n",
    "            batch = list(self.buffer)\n",
    "            self.buffer.clear()\n",
    "            self.last_flush_ts = now\n",
    "\n",
    "            try:\n",
    "                print(f\"[JDBC] flushing {len(batch)} rows\")\n",
    "                execute_batch(self.cursor, self.sql, batch)\n",
    "                self.conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error flushing batch: {e}\")\n",
    "                # Put rows back so they're not lost\n",
    "                self.buffer[:0] = batch\n",
    "                self.conn.rollback()\n",
    "                raise\n",
    "\n",
    "        def _write_immediate(self, values):\n",
    "            \"\"\"Write single row immediately (simple mode only)\"\"\"\n",
    "            try:\n",
    "                print(f\"[JDBC] writing row immediately\")\n",
    "                self.cursor.execute(self.sql, values)\n",
    "                self.conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error writing row: {e}\")\n",
    "                self.conn.rollback()\n",
    "                raise\n",
    "\n",
    "        def process(self, row):\n",
    "            # Initialize schema on first row\n",
    "            self._initialize_schema(row)\n",
    "            \n",
    "            print(f\"[JDBC] process row: {row}\")\n",
    "\n",
    "            # Extract values dynamically based on column order\n",
    "            row_dict = row.asDict()\n",
    "            values = []\n",
    "            \n",
    "            for col_name in self.column_names:\n",
    "                val = row_dict[col_name]\n",
    "                \n",
    "                # Convert datetime objects to ISO format strings\n",
    "                if isinstance(val, datetime):\n",
    "                    val = val.isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "                \n",
    "                values.append(val)\n",
    "            \n",
    "            if use_buffered_mode:\n",
    "                # Buffered mode: add to buffer and flush if needed\n",
    "                self.buffer.append(tuple(values))\n",
    "                self._flush_if_needed(force=False)\n",
    "            else:\n",
    "                # Simple mode: write immediately\n",
    "                self._write_immediate(tuple(values))\n",
    "\n",
    "        def close(self, error):\n",
    "            print(f\"[JDBC] close, error={error}\")\n",
    "            try:\n",
    "                if error is None and use_buffered_mode:\n",
    "                    # Flush any remaining buffered rows\n",
    "                    self._flush_if_needed(force=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[JDBC] Error flushing on close: {e}\")\n",
    "            try:\n",
    "                if getattr(self, \"cursor\", None):\n",
    "                    self.cursor.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if getattr(self, \"conn\", None):\n",
    "                    self.conn.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return PgForeachWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14e89ee-09e1-4ae8-824f-d8d6de6aa7b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "# Get widget parameters\n",
    "max_batch_size_param = int(dbutils.widgets.get(\"max_batch_size\"))\n",
    "flush_secs_param = float(dbutils.widgets.get(\"flush_secs\"))\n",
    "\n",
    "print(f\"Buffered writer configuration:\")\n",
    "print(f\"  max_batch_size: {max_batch_size_param}\")\n",
    "print(f\"  flush_secs: {flush_secs_param}\")\n",
    "print()\n",
    "\n",
    "jdbc_writer = make_pg_buffered_writer(\n",
    "    jdbc_url=JDBC_URL,\n",
    "    jdbc_user=JDBC_USER,\n",
    "    jdbc_password=JDBC_PASSWORD,\n",
    "    jdbc_driver=JDBC_DRIVER,\n",
    "    table_name=TABLE_NAME,\n",
    "    pool_size=None,\n",
    "    max_batch_size=max_batch_size_param,\n",
    "    flush_secs=flush_secs_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25f0751-9ffb-4994-8550-d31deeb602d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create or recreate PostgreSQL target table"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from urllib.parse import urlparse\n",
    "from pyspark.sql.types import StringType, TimestampType, IntegerType, LongType, DoubleType, FloatType, BooleanType, DateType\n",
    "\n",
    "# Parse JDBC URL to get connection parameters\n",
    "def parse_pg_url(jdbc_url, jdbc_user, jdbc_password):\n",
    "    url = jdbc_url.replace(\"jdbc:\", \"\")\n",
    "    parsed = urlparse(url)\n",
    "    return {\n",
    "        \"host\": parsed.hostname,\n",
    "        \"port\": parsed.port or 5432,\n",
    "        \"dbname\": parsed.path.lstrip(\"/\"),\n",
    "        \"user\": jdbc_user,\n",
    "        \"password\": jdbc_password,\n",
    "    }\n",
    "\n",
    "# Map Spark types to PostgreSQL types\n",
    "def spark_to_postgres_type(spark_type):\n",
    "    if isinstance(spark_type, StringType):\n",
    "        return \"TEXT\"\n",
    "    elif isinstance(spark_type, TimestampType):\n",
    "        return \"TIMESTAMP\"\n",
    "    elif isinstance(spark_type, DateType):\n",
    "        return \"DATE\"\n",
    "    elif isinstance(spark_type, IntegerType):\n",
    "        return \"INTEGER\"\n",
    "    elif isinstance(spark_type, LongType):\n",
    "        return \"BIGINT\"\n",
    "    elif isinstance(spark_type, DoubleType) or isinstance(spark_type, FloatType):\n",
    "        return \"DOUBLE PRECISION\"\n",
    "    elif isinstance(spark_type, BooleanType):\n",
    "        return \"BOOLEAN\"\n",
    "    else:\n",
    "        return \"TEXT\"  # Default fallback\n",
    "\n",
    "conn_kwargs = parse_pg_url(JDBC_URL, JDBC_USER, JDBC_PASSWORD)\n",
    "\n",
    "print(f\"Connecting to PostgreSQL database: {conn_kwargs['dbname']}\")\n",
    "print(f\"Target table: {TABLE_NAME}\")\n",
    "print()\n",
    "\n",
    "# Get schema from the streaming DataFrame\n",
    "print(\"Inspecting streaming DataFrame schema...\")\n",
    "df_schema = events_df.schema\n",
    "\n",
    "print(\"DataFrame columns:\")\n",
    "for field in df_schema.fields:\n",
    "    print(f\"  - {field.name}: {field.dataType}\")\n",
    "print()\n",
    "\n",
    "# Build CREATE TABLE statement dynamically\n",
    "column_definitions = []\n",
    "for field in df_schema.fields:\n",
    "    col_name = field.name\n",
    "    pg_type = spark_to_postgres_type(field.dataType)\n",
    "    nullable = \"\" if field.nullable else \"NOT NULL\"\n",
    "    column_definitions.append(f\"{col_name} {pg_type} {nullable}\".strip())\n",
    "\n",
    "create_sql = f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    {',\\n    '.join(column_definitions)}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Connect and create/recreate table\n",
    "try:\n",
    "    conn = psycopg2.connect(**conn_kwargs)\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop table if exists\n",
    "    drop_sql = f\"DROP TABLE IF EXISTS {TABLE_NAME}\"\n",
    "    print(f\"Executing: {drop_sql}\")\n",
    "    cursor.execute(drop_sql)\n",
    "    print(\"  ✓ Table dropped (if existed)\")\n",
    "    print()\n",
    "    \n",
    "    # Create table with schema matching DataFrame\n",
    "    print(f\"Executing: {create_sql}\")\n",
    "    cursor.execute(create_sql)\n",
    "    print(\"  ✓ Table created successfully\")\n",
    "    print()\n",
    "    \n",
    "    # Verify table exists\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT column_name, data_type \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_name = '{TABLE_NAME}'\n",
    "        ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    print(\"PostgreSQL table schema:\")\n",
    "    for col_name, col_type in columns:\n",
    "        print(f\"  - {col_name}: {col_type}\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print()\n",
    "    print(\"✓ Table ready for streaming writes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1587fa55-b6b1-463e-99cf-07eaf5892a18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming query with optional deduplication"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Re-read widget parameters to ensure we use the latest values\n",
    "print(\"Reading widget parameters...\")\n",
    "deduplicate = dbutils.widgets.get(\"Deduplicate\")\n",
    "max_batch_size_param = int(dbutils.widgets.get(\"max_batch_size\"))\n",
    "flush_secs_param = float(dbutils.widgets.get(\"flush_secs\"))\n",
    "\n",
    "print(f\"Widget values:\")\n",
    "print(f\"  Deduplicate: {deduplicate}\")\n",
    "print(f\"  max_batch_size: {max_batch_size_param}\")\n",
    "print(f\"  flush_secs: {flush_secs_param}\")\n",
    "print()\n",
    "\n",
    "# Recreate the JDBC writer with current widget values\n",
    "print(\"Creating JDBC writer with current parameters...\")\n",
    "jdbc_writer = make_pg_buffered_writer(\n",
    "    jdbc_url=JDBC_URL,\n",
    "    jdbc_user=JDBC_USER,\n",
    "    jdbc_password=JDBC_PASSWORD,\n",
    "    jdbc_driver=JDBC_DRIVER,\n",
    "    table_name=TABLE_NAME,\n",
    "    pool_size=None,\n",
    "    max_batch_size=max_batch_size_param,\n",
    "    flush_secs=flush_secs_param\n",
    ")\n",
    "print(\"  ✓ JDBC writer created\")\n",
    "print()\n",
    "\n",
    "df_for_write = events_df  # Start with the base streaming DataFrame\n",
    "\n",
    "if deduplicate == \"Yes\":\n",
    "    checkpoint_path = \"/tmp/foreach_jdbc_checkpoint_deduplicated\"\n",
    "    \n",
    "    print(\"Executing streaming query WITH deduplication...\")\n",
    "    print(\"  - Parsing JSON to extract amount and daily_average\")\n",
    "    print(\"  - Adding 10-minute watermark\")\n",
    "    print(\"  - Dropping duplicates based on customerID, amount, and daily_average\")\n",
    "    print()\n",
    "    \n",
    "    # Delete checkpoint before starting\n",
    "    print(f\"Deleting checkpoint location: {checkpoint_path}\")\n",
    "    try:\n",
    "        dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "        print(\"  Checkpoint deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Checkpoint deletion skipped (may not exist): {e}\")\n",
    "    print()\n",
    "    \n",
    "    query = (\n",
    "        df_for_write    \n",
    "        .withColumn(\"parsed_json\", F.from_json(\"value_json\", \"amount STRING, daily_average STRING\"))\n",
    "        .withColumn(\"amount\", F.col(\"parsed_json.amount\"))\n",
    "        .withColumn(\"daily_average\", F.col(\"parsed_json.daily_average\"))\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "        .dropDuplicates([\"customerID\", \"amount\", \"daily_average\"])\n",
    "        .select(\"customerID\", \"value_json\", \"timestamp\")\n",
    "        .writeStream\n",
    "        .foreach(jdbc_writer)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(\"jdbc_sink_writer_deduplicated\")\n",
    "        .trigger(realTime=\"5 minutes\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    print(f\"Deduplicated streaming query started: {query.id}\")\n",
    "    \n",
    "else:\n",
    "    checkpoint_path = \"/tmp/foreach_jdbc_checkpoint_standard\"\n",
    "    \n",
    "    print(\"Executing streaming query WITHOUT deduplication...\")\n",
    "    print()\n",
    "    \n",
    "    # Delete checkpoint before starting\n",
    "    print(f\"Deleting checkpoint location: {checkpoint_path}\")\n",
    "    try:\n",
    "        dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "        print(\"  Checkpoint deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Checkpoint deletion skipped (may not exist): {e}\")\n",
    "    print()\n",
    "    \n",
    "    query = (\n",
    "        df_for_write\n",
    "        .writeStream\n",
    "        .foreach(jdbc_writer)\n",
    "        .outputMode(\"update\")\n",
    "        .queryName(\"jdbc_sink_writer\")\n",
    "        .trigger(realTime=\"5 minutes\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .start()\n",
    "    )\n",
    "    \n",
    "    print(f\"Standard streaming query started: {query.id}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2-read-kafka-store-to-lakebase-realtime",
   "widgets": {
    "Deduplicate": {
     "currentValue": "No",
     "nuid": "531ebbba-9f6c-4817-92a1-c17f61a7e360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "No",
      "label": null,
      "name": "Deduplicate",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "No",
        "Yes"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "No",
      "label": null,
      "name": "Deduplicate",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "No",
        "Yes"
       ]
      }
     }
    },
    "flush_secs": {
     "currentValue": "1",
     "nuid": "faf16f4a-9ec6-46f5-8ba0-1516118f835d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5.0",
      "label": "Flush Seconds",
      "name": "flush_secs",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5.0",
      "label": "Flush Seconds",
      "name": "flush_secs",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "max_batch_size": {
     "currentValue": "10000",
     "nuid": "0b44022a-6dbb-4d7b-a058-364fdbbe1ce0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "500",
      "label": "Max Batch Size",
      "name": "max_batch_size",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "500",
      "label": "Max Batch Size",
      "name": "max_batch_size",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
